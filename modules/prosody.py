import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import torch
import torch.nn as nn
import opensmile

from transformers import Wav2Vec2Processor
from transformers.models.wav2vec2.modeling_wav2vec2 import (
    Wav2Vec2Model,
    Wav2Vec2PreTrainedModel,
)

import config


class RegressionHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.final_dropout)
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, features):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class EmotionModel(Wav2Vec2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config
        self.wav2vec2 = Wav2Vec2Model(config)
        self.classifier = RegressionHead(config)
        self.init_weights()

    def forward(self, input_values: torch.Tensor):
        outputs = self.wav2vec2(input_values)
        hidden_states = outputs[0]
        hidden_states = torch.mean(hidden_states, dim=1)
        logits = self.classifier(hidden_states)
        return hidden_states, logits


class ProsodyAnalyzer:
    """
    - 1ë‹¨ê³„: analyze()
        - word ë‹¨ìœ„ë¡œ prosodyë¥¼ ê³„ì‚°
        - fallback ê¸°ì¤€ìœ¼ë¡œ [0,1] ì •ê·œí™”
        - ë™ì‹œì— *_raw ê°’ë„ ê°™ì´ ì €ì¥
    - 2ë‹¨ê³„: apply_presets_in_place()
        - Labelerê°€ wordë§ˆë‹¤ speakerë¥¼ ë¶™ì¸ ë’¤ í˜¸ì¶œ
        - preset JSON + speaker ì´ë¦„ì„ ì´ìš©í•´
          loudness/valence/arousalì„ casterë³„ë¡œ ë‹¤ì‹œ ì •ê·œí™”í•´ì„œ ë®ì–´ì”€
    """

    def __init__(
        self,
        sample_rate: int = 16000,
        device: Optional[str] = None,
        min_segment_sec: float = 0.2,
        min_word_window_sec: float = 1.0,
    ):
        self.sample_rate = sample_rate
        self.min_segment_sec = float(min_segment_sec)
        self.min_word_window_sec = float(min_word_window_sec)

        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        model_name = "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"
        self.processor = Wav2Vec2Processor.from_pretrained(model_name)
        self.model = EmotionModel.from_pretrained(model_name).to(self.device).eval()

        self.smile = opensmile.Smile(
            feature_set=opensmile.FeatureSet.eGeMAPSv02,
            feature_level=opensmile.FeatureLevel.LowLevelDescriptors,
        )

        # preset ê´€ë ¨ ìºì‹œ
        self._presets: Optional[Dict[str, Any]] = None  # JSON ê·¸ëŒ€ë¡œ
        self._name_to_sid: Optional[Dict[str, int]] = None  # caster ì´ë¦„ -> preset ID

    # ------------------------------------------------------------------
    # ë‚´ë¶€ ìœ í‹¸: loudness / emotion fallback ì •ê·œí™”
    # ------------------------------------------------------------------
    def _fallback_normalize_loudness(self, loudness_raw: float) -> float:
        """
        opensmile Loudness_sma3 í‰ê· ê°’ ê¸°ì¤€ ëŒ€ëµì ì¸ [0,1] ë§¤í•‘.
        -60dB ~ 0dB ì •ë„ë¥¼ 0~1ë¡œ ë§µí•‘í•œë‹¤ê³  ê°€ì •.
        """
        if np.isnan(loudness_raw):
            return 0.5
        val = (loudness_raw + 60.0) / 60.0
        return float(np.clip(val, 0.0, 1.0))

    def _fallback_normalize_emotion(self, value_raw: float) -> float:
        """
        MSP-DIM íšŒê·€ ì¶œë ¥ì´ ëŒ€ëµ [-1, 1] ê·¼ì²˜ë¼ê³  ê°€ì •í•˜ê³  [0,1]ë¡œ ì„ í˜• ë§¤í•‘.
        """
        if np.isnan(value_raw):
            return 0.5
        val = (value_raw + 1.0) / 2.0
        return float(np.clip(val, 0.0, 1.0))

    # ------------------------------------------------------------------
    # preset JSON ë¡œë”© + ì´ë¦„ -> ID ë§¤í•‘
    # ------------------------------------------------------------------
    def _load_presets(self) -> Dict[str, Any]:
        """
        config.CASTER_PRESET_JSON ì—ì„œ preset JSONì„ í•œ ë²ˆë§Œ ë¡œë”©.
        JSON êµ¬ì¡°(ì˜ˆì‹œ):

        {
          "1": {
            "id": 1,
            "name": "DRAKOS",
            "samples": {"words": 1234},
            "loudness": {"p05": ..., "p50": ..., "p95": ...},
            "arousal": {"p05": ..., "p50": ..., "p95": ...},
            "valence": {"low": ..., "center": ..., "high": ...}
          },
          "2": { ... }
        }
        """
        if self._presets is not None:
            return self._presets

        path = getattr(config, "CASTER_PRESET_JSON", None)
        if not path:
            print("[prosody] CASTER_PRESET_JSON not set in config. Using fallback normalization only.")
            self._presets = {}
            self._name_to_sid = None
            return self._presets

        p = Path(path)
        if not p.is_file():
            print(f"[prosody] Preset JSON not found: {p}. Using fallback normalization only.")
            self._presets = {}
            self._name_to_sid = None
            return self._presets

        try:
            with p.open("r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"[prosody] Failed to load presets from {p}: {e}. Using fallback only.")
            data = {}

        self._presets = data

        # caster ì´ë¦„ -> speaker_id ë§¤í•‘ ìƒì„±
        name_to_sid: Dict[str, int] = {}
        for key, entry in data.items():
            try:
                sid = int(key)
            except Exception:
                continue
            if not isinstance(entry, dict):
                continue
            name = entry.get("name")
            if not name:
                continue
            name_to_sid[str(name)] = sid
        self._name_to_sid = name_to_sid if name_to_sid else None

        print(f"[prosody] Loaded {len(data)} caster presets from {p}")
        return self._presets

    def _speaker_name_to_id(self, speaker_name: Optional[str]) -> Optional[int]:
        """
        word["speaker"] ì— ë“¤ì–´ìˆëŠ” ì´ë¦„(DRAKOS ë“±)ì„ preset JSONê³¼ ë§¤ì¹­í•´ì„œ ID ë°˜í™˜.
        """
        if not speaker_name:
            return None
        if self._presets is None:
            self._load_presets()
        if not self._name_to_sid:
            return None
        return self._name_to_sid.get(str(speaker_name))

    # ------------------------------------------------------------------
    # preset ê¸°ë°˜ ì •ê·œí™” (ID ì—†ìœ¼ë©´ fallback ì‚¬ìš©)
    # ------------------------------------------------------------------
    def _normalize_with_preset(
        self,
        loudness_raw: float,
        valence_raw: float,
        arousal_raw: float,
        speaker_id: Optional[int],
    ) -> (float, float, float):
        """
        - speaker_id ê°€ ì£¼ì–´ì§€ê³  presetì´ ìˆìœ¼ë©´ preset ê¸°ë°˜ ì •ê·œí™”
        - ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ fallback ì •ê·œí™”
        """
        presets = self._load_presets()
        if not presets or speaker_id is None:
            # preset ì—†ê±°ë‚˜ speaker_id ëª¨ë¥´ë©´ fallback
            return (
                self._fallback_normalize_loudness(loudness_raw),
                self._fallback_normalize_emotion(valence_raw),
                self._fallback_normalize_emotion(arousal_raw),
            )

        entry = presets.get(str(speaker_id))
        if not isinstance(entry, dict):
            return (
                self._fallback_normalize_loudness(loudness_raw),
                self._fallback_normalize_emotion(valence_raw),
                self._fallback_normalize_emotion(arousal_raw),
            )

        # --- loudness: p05 ~ p95 êµ¬ê°„ì„ 0~1ë¡œ ë§¤í•‘ ----------------------
        loud_cfg = entry.get("loudness", {})
        lp05 = float(loud_cfg.get("p05", -60.0))
        lp95 = float(loud_cfg.get("p95", 0.0))
        if lp95 <= lp05:
            loud_norm = self._fallback_normalize_loudness(loudness_raw)
        else:
            loud_norm = (loudness_raw - lp05) / (lp95 - lp05)
            loud_norm = float(np.clip(loud_norm, 0.0, 1.0))

        # --- arousal: p05 ~ p95 êµ¬ê°„ì„ 0~1ë¡œ ë§¤í•‘ ----------------------
        aro_cfg = entry.get("arousal", {})
        ap05 = float(aro_cfg.get("p05", -1.0))
        ap95 = float(aro_cfg.get("p95", 1.0))
        if ap95 <= ap05:
            aro_norm = self._fallback_normalize_emotion(arousal_raw)
        else:
            aro_norm = (arousal_raw - ap05) / (ap95 - ap05)
            aro_norm = float(np.clip(aro_norm, 0.0, 1.0))

        # --- valence: low/center/high ê¸°ì¤€ìœ¼ë¡œ -1~1 â†’ 0~1 ë§¤í•‘ ----------
        val_cfg = entry.get("valence", {})
        v_low = float(val_cfg.get("low", -1.0))
        v_center = float(val_cfg.get("center", 0.0))
        v_high = float(val_cfg.get("high", 1.0))

        if not (v_low < v_center < v_high):
            val_norm = self._fallback_normalize_emotion(valence_raw)
        else:
            if valence_raw <= v_center:
                # low ~ center êµ¬ê°„ì„ 0~0.5ë¡œ
                val_norm = 0.0 + 0.5 * (valence_raw - v_low) / max(v_center - v_low, 1e-6)
            else:
                # center ~ high êµ¬ê°„ì„ 0.5~1.0ìœ¼ë¡œ
                val_norm = 0.5 + 0.5 * (valence_raw - v_center) / max(v_high - v_center, 1e-6)
            val_norm = float(np.clip(val_norm, 0.0, 1.0))

        return loud_norm, val_norm, aro_norm

    # ------------------------------------------------------------------
    # chunk ë‹¨ìœ„ prosody ê³„ì‚°
    # ------------------------------------------------------------------
    def _analyze_chunk_emotion(self, chunk: np.ndarray) -> Dict[str, float]:
        if chunk.ndim > 1:
            chunk = np.mean(chunk, axis=0)
        x = np.asarray(chunk, dtype=np.float32)

        proc = self.processor(x, sampling_rate=self.sample_rate)
        inp = proc["input_values"][0].astype(np.float32)
        inp_t = torch.from_numpy(inp).unsqueeze(0).to(self.device)

        with torch.inference_mode():
            _, logits = self.model(inp_t)

        logits_np = logits.squeeze(0).detach().cpu().numpy()
        arousal = float(logits_np[0])
        valence = float(logits_np[2])

        return {
            "valence": valence,
            "arousal": arousal,
        }

    def _analyze_chunk_loudness(self, chunk: np.ndarray) -> float:
        if chunk.ndim > 1:
            chunk = np.mean(chunk, axis=0)
        chunk = np.asarray(chunk, dtype=np.float32)
        df = self.smile.process_signal(chunk, self.sample_rate)
        if "Loudness_sma3" not in df.columns:
            return float("nan")
        loudness = float(df["Loudness_sma3"].mean())
        return loudness

    # ------------------------------------------------------------------
    # STT segmentì—ì„œ word ë¦¬ìŠ¤íŠ¸ ëª¨ìœ¼ê¸°
    # ------------------------------------------------------------------
    def _collect_words(self, segments: List[Any]) -> List[Dict[str, Any]]:
        words: List[Dict[str, Any]] = []
        for seg in segments:
            seg_words = getattr(seg, "words", None)
            if not seg_words:
                continue
            for w in seg_words:
                start = getattr(w, "start", None)
                end = getattr(w, "end", None)
                text = getattr(w, "word", None) or getattr(w, "text", None)
                if start is None or end is None or text is None:
                    continue
                start = float(start)
                end = float(end)
                if end <= start:
                    continue
                words.append(
                    {
                        "start": start,
                        "end": end,
                        "text": text,
                    }
                )
        return words

    # ------------------------------------------------------------------
    # word ë‹¨ìœ„ prosody ë¶„ì„ (1ì°¨: raw + fallback ì •ê·œí™”)
    # ------------------------------------------------------------------
    def _analyze_words(
        self,
        audio: np.ndarray,
        words: List[Dict[str, Any]],
        speaker_id: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        results: List[Dict[str, Any]] = []
        n = len(audio)
        sr = self.sample_rate
        min_win = self.min_word_window_sec

        for w in words:
            ws = float(w["start"])
            we = float(w["end"])
            text = w.get("text")

            center = 0.5 * (ws + we)
            dur = we - ws

            if dur >= min_win:
                win_start = ws
                win_end = we
            else:
                half = 0.5 * min_win
                win_start = center - half
                win_end = center + half

            if win_start < 0.0:
                shift = -win_start
                win_start += shift
                win_end += shift

            win_start_idx = int(win_start * sr)
            win_end_idx = int(win_end * sr)

            win_start_idx = max(0, min(win_start_idx, n))
            win_end_idx = max(win_start_idx + 1, min(win_end_idx, n))

            chunk = audio[win_start_idx:win_end_idx]

            loudness_raw = self._analyze_chunk_loudness(chunk)
            emo_vals = self._analyze_chunk_emotion(chunk)
            valence_raw = float(emo_vals["valence"])
            arousal_raw = float(emo_vals["arousal"])

            # ğŸ”¹ 1ì°¨ ë¶„ì„ ì‹œì ì—ëŠ” speaker_id ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ
            #    speaker_id=None ì„ ë„£ê³  -> fallback ê¸°ì¤€ìœ¼ë¡œë§Œ ì •ê·œí™”
            loud_norm, val_norm, aro_norm = self._normalize_with_preset(
                loudness_raw,
                valence_raw,
                arousal_raw,
                speaker_id=speaker_id,
            )

            results.append(
                {
                    "start": ws,
                    "end": we,
                    "text": text,
                    # í˜„ì¬ overlayì—ì„œ ë°”ë¡œ ì“¸ ê°’ (fallback ê¸°ì¤€ [0,1])
                    "loudness": float(loud_norm),
                    "valence": float(val_norm),
                    "arousal": float(aro_norm),
                    # ë‚˜ì¤‘ì— caster preset ì ìš©í•  ë•Œ ì“¸ raw ê°’
                    "loudness_raw": float(loudness_raw),
                    "valence_raw": float(valence_raw),
                    "arousal_raw": float(arousal_raw),
                }
            )

        return results

    # ------------------------------------------------------------------
    # public API: prosody 1ì°¨ ë¶„ì„
    # ------------------------------------------------------------------
    def analyze(
        self,
        audio: np.ndarray,
        segments: List[Any],
        speaker_id: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        - ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ ì—”íŠ¸ë¦¬.
        - speaker_id ëŠ” (í•„ìš”í•˜ë‹¤ë©´) ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•œ ëª…ìœ¼ë¡œ ê°€ì •í•  ë•Œ ì“¸ ìˆ˜ ìˆëŠ” ì˜µì…˜.
          ì§€ê¸ˆ êµ¬ì¡°ì—ì„œëŠ” ëŒ€ë¶€ë¶„ None ìœ¼ë¡œ í˜¸ì¶œë  ê²ƒì„.
        """
        audio = np.asarray(audio, dtype=np.float32)
        if audio.ndim > 1:
            audio = np.mean(audio, axis=-1)

        word_list = self._collect_words(segments)
        word_level = self._analyze_words(audio, word_list, speaker_id=speaker_id) if word_list else []

        return {
            "words": word_level,
        }

    # ------------------------------------------------------------------
    # public API: Labeler ì´í›„ì— preset ê¸°ë°˜ ì¬ì •ê·œí™”
    # ------------------------------------------------------------------
    def apply_presets_in_place(self, prosody_info: Dict[str, Any]) -> None:
        """
        Labeler.assign_labels() ì´í›„ì— í˜¸ì¶œí•´ì£¼ëŠ” í•¨ìˆ˜.

        - ê°€ì •:
          prosody_info["words"][i] ì—ëŠ” ì´ë¯¸ ë‹¤ìŒ í•„ë“œê°€ ë“¤ì–´ ìˆìŒ
            - "loudness_raw", "valence_raw", "arousal_raw"  (analyze ë‹¨ê³„ì—ì„œ ì €ì¥)
            - "speaker"  (Labelerê°€ ë¶™ì¸ caster ì´ë¦„)
        - ë™ì‘:
          ê° wordì˜ speaker ì´ë¦„ì„ preset JSONì˜ nameê³¼ ë§¤ì¹­í•´ì„œ
          loudness/valence/arousal ì„ casterë³„ preset ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ê·œí™”í•˜ê³  ë®ì–´ì”€.
        """
        if not isinstance(prosody_info, dict):
            return
        words = prosody_info.get("words")
        if not words:
            return

        presets = self._load_presets()
        if not presets or not self._name_to_sid:
            # preset íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•„ë¬´ ê²ƒë„ ì•ˆ í•˜ê³  ì¢…ë£Œ
            return

        for w in words:
            speaker_name = w.get("speaker")
            speaker_id = self._speaker_name_to_id(speaker_name)

            loudness_raw = float(w.get("loudness_raw", w.get("loudness", 0.0)))
            valence_raw = float(w.get("valence_raw", w.get("valence", 0.0)))
            arousal_raw = float(w.get("arousal_raw", w.get("arousal", 0.0)))

            loud_norm, val_norm, aro_norm = self._normalize_with_preset(
                loudness_raw,
                valence_raw,
                arousal_raw,
                speaker_id=speaker_id,
            )

            # ìµœì¢…ì ìœ¼ë¡œ overlayì—ì„œ ì‚¬ìš©í•  ê°’ ë®ì–´ì“°ê¸°
            w["loudness"] = float(loud_norm)
            w["valence"] = float(val_norm)
            w["arousal"] = float(aro_norm)
